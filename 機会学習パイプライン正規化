# æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®æ­£è¦åŒ–
## ğŸ“‹ æ­£ã—ã„å‡¦ç†ãƒ•ãƒ­ãƒ¼å›³ - æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ä¸å¯§è§£èª¬

---

## ğŸ¯ ã¯ã˜ã‚ã«ï¼šãªãœé †åºãŒé‡è¦ãªã®ã‹ï¼Ÿ

æ©Ÿæ¢°å­¦ç¿’ã«ãŠã„ã¦ã€**ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†é †åº**ã¯æˆåŠŸã®éµã‚’æ¡ã‚Šã¾ã™ã€‚ç‰¹ã«æ­£è¦åŒ–ã®é †åºã‚’é–“é•ãˆã‚‹ã¨ã€**ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯**ã¨ã„ã†æ·±åˆ»ãªå•é¡ŒãŒç™ºç”Ÿã—ã€å®Ÿéš›ã«ã¯ä½¿ãˆãªã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã£ã¦ã—ã¾ã„ã¾ã™ã€‚

### ğŸš¨ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã¨ã¯ï¼Ÿ
```
ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ = æœ¬æ¥çŸ¥ã£ã¦ã¯ã„ã‘ãªã„ã€Œæœªæ¥ã®æƒ…å ±ã€ãŒè¨“ç·´ã«ç´›ã‚Œè¾¼ã‚€ã“ã¨

ä¾‹ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ï¼ˆå¹³å‡ãƒ»æ¨™æº–åå·®ï¼‰ã‚’ä½¿ã£ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
â†’ å®Ÿéš›ã®é‹ç”¨ã§ã¯å¾—ã‚‰ã‚Œãªã„æƒ…å ±ã‚’ä½¿ã£ã¦ã—ã¾ã†
â†’ éåº¦ã«æ¥½è¦³çš„ãªè©•ä¾¡çµæœãŒå‡ºã‚‹
â†’ æœ¬ç•ªã§æ€§èƒ½ãŒå¤§å¹…ã«æ‚ªåŒ–ã™ã‚‹
```

---

## ğŸ”„ å®Œå…¨ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ğŸ“Š æ­£ã—ã„å‡¦ç†ãƒ•ãƒ­ãƒ¼å›³

```
ã€ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ•ã‚§ãƒ¼ã‚ºã€‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç”Ÿãƒ‡ãƒ¼ã‚¿       â”‚ â† CSVãƒ•ã‚¡ã‚¤ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€APIç­‰
â”‚ (CSV, DB, etc.) â”‚   ã‹ã‚‰å–å¾—ã—ãŸæœªåŠ å·¥ã®ãƒ‡ãƒ¼ã‚¿
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â”‚ â† æ¬ æå€¤å‡¦ç†ã€ç•°å¸¸å€¤é™¤å»ã€
â”‚ (æ¬ æå€¤å‡¦ç†ç­‰)    â”‚   ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã€é‡è¤‡é™¤å»
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²  â”‚ â† train_test_split()ã§
â”‚ train_test_split â”‚   ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—é †ã«åˆ†å‰²
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
ã€æ­£è¦åŒ–ãƒ•ã‚§ãƒ¼ã‚ºã€‘âš ï¸ é‡è¦ï¼šã“ã®é †åºã‚’å®ˆã‚‹ï¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â‘  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§   â”‚ â† fit()ãƒ¡ã‚½ãƒƒãƒ‰ã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰
â”‚   çµ±è¨ˆå€¤ã‚’å­¦ç¿’   â”‚   å¹³å‡ãƒ»æ¨™æº–åå·®ãƒ»æœ€å°å€¤ãƒ»æœ€å¤§å€¤ã‚’è¨ˆç®—
â”‚   scaler.fit()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â‘¡ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’   â”‚ â† transform()ãƒ¡ã‚½ãƒƒãƒ‰ã§â‘ ã§å­¦ç¿’ã—ãŸ
â”‚   å¤‰æ›           â”‚   çµ±è¨ˆå€¤ã‚’ä½¿ã£ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
â”‚ scaler.transform â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â‘¢ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ â”‚ â† åŒã˜scalerã®transform()ãƒ¡ã‚½ãƒƒãƒ‰ã§
â”‚   åŒã˜çµ±è¨ˆå€¤ã§   â”‚   â‘ ã§å­¦ç¿’ã—ãŸçµ±è¨ˆå€¤ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å¤‰æ›
â”‚   å¤‰æ›           â”‚   ï¼ˆæ–°ãŸã«çµ±è¨ˆå€¤ã¯è¨ˆç®—ã—ãªã„ï¼‰
â”‚ scaler.transform â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»è©•ä¾¡ã€‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ãƒ¢ãƒ‡ãƒ«è¨“ç·´       â”‚ â† æ­£è¦åŒ–æ¸ˆã¿è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡       â”‚ â† æ­£è¦åŒ–æ¸ˆã¿ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½è©•ä¾¡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ å„ãƒ•ã‚§ãƒ¼ã‚ºã®è©³ç´°å®Ÿè£…

### ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ‡ãƒ¼ã‚¿æº–å‚™

```python
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd                           # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œ
import numpy as np                            # æ•°å€¤è¨ˆç®—
from sklearn.model_selection import train_test_split  # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
from sklearn.preprocessing import StandardScaler      # æ¨™æº–åŒ–
from sklearn.ensemble import RandomForestRegressor    # ãƒ¢ãƒ‡ãƒ«ä¾‹
from sklearn.metrics import mean_squared_error        # è©•ä¾¡æŒ‡æ¨™
import matplotlib.pyplot as plt               # å¯è¦–åŒ–

# 1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
print("=== ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ ===")
df = pd.read_csv('employee_data.csv')        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape}")            # è¡Œæ•°ãƒ»åˆ—æ•°ç¢ºèª
print("æœ€åˆã®5è¡Œ:")
print(df.head())                              # ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ç¢ºèª

# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±è¡¨ç¤º
print("\nãƒ‡ãƒ¼ã‚¿å‹æƒ…å ±:")
print(df.dtypes)                              # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹
print("\næ¬ æå€¤ã®ç¢ºèª:")
print(df.isnull().sum())                      # å„åˆ—ã®æ¬ æå€¤æ•°

# 2. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ ===")

# æ¬ æå€¤å‡¦ç†ã®ä¾‹
if df.isnull().sum().sum() > 0:               # æ¬ æå€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
    print("æ¬ æå€¤ã‚’ç™ºè¦‹ã€‚å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™...")
    # æ•°å€¤åˆ—ã¯å¹³å‡å€¤ã§è£œå®Œ
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if df[col].isnull().sum() > 0:        # è©²å½“åˆ—ã«æ¬ æå€¤ãŒã‚ã‚‹å ´åˆ
            mean_value = df[col].mean()       # å¹³å‡å€¤ã‚’è¨ˆç®—
            df[col].fillna(mean_value, inplace=True)  # å¹³å‡å€¤ã§è£œå®Œ
            print(f"{col}åˆ—ã®æ¬ æå€¤ã‚’å¹³å‡å€¤{mean_value:.2f}ã§è£œå®Œ")

# é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®é™¤å»
duplicates = df.duplicated().sum()            # é‡è¤‡è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
if duplicates > 0:
    df = df.drop_duplicates()                 # é‡è¤‡è¡Œã‚’å‰Šé™¤
    print(f"{duplicates}è¡Œã®é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤")

print(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.shape}")
```

### ãƒ•ã‚§ãƒ¼ã‚º2: è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰²

```python
print("\n=== ãƒ•ã‚§ãƒ¼ã‚º2: è¨“ç·´/ãƒ†ã‚¹ãƒˆåˆ†å‰² ===")

# ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’åˆ†é›¢
# drop()ãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’é™¤ã„ãŸç‰¹å¾´é‡ã‚’ä½œæˆ
X = df.drop('salary', axis=1)                # salaryã‚’é™¤ãå…¨åˆ—ãŒç‰¹å¾´é‡
y = df['salary']                             # salaryãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°

print(f"ç‰¹å¾´é‡ã®å½¢çŠ¶: {X.shape}")             # ç‰¹å¾´é‡ã®è¡Œæ•°ãƒ»åˆ—æ•°
print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶: {y.shape}")         # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®è¦ç´ æ•°

# è¨“ç·´/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
# train_test_split()ã§8:2ã®æ¯”ç‡ã§åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y,                                    # åˆ†å‰²å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿
    test_size=0.2,                          # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆ20%ï¼‰
    random_state=42,                        # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰
    stratify=None                           # å›å¸°å•é¡Œãªã®ã§å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãªã—
)

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_train.shape[0]}è¡Œ")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_test.shape[0]}è¡Œ")

# åˆ†å‰²å¾Œã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒç¢ºèª
print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµ±è¨ˆ:")
print(f"å¹³å‡: {y_train.mean():.2f}")          # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å¹´å
print(f"æ¨™æº–åå·®: {y_train.std():.2f}")       # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åå·®

print(f"\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµ±è¨ˆ:")
print(f"å¹³å‡: {y_test.mean():.2f}")           # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å¹´å
print(f"æ¨™æº–åå·®: {y_test.std():.2f}")        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åå·®
```

### ãƒ•ã‚§ãƒ¼ã‚º3: æ­£è¦åŒ–ï¼ˆé‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼‰

```python
print("\n=== ãƒ•ã‚§ãƒ¼ã‚º3: æ­£è¦åŒ–ï¼ˆé‡è¦ãƒ•ã‚§ãƒ¼ã‚ºï¼‰===")

# æ•°å€¤ç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã¯åˆ¥å‡¦ç†ï¼‰
numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()
print(f"æ­£è¦åŒ–å¯¾è±¡ã®æ•°å€¤ç‰¹å¾´é‡: {numeric_features}")

# âš ï¸ é‡è¦ï¼šStandardScalerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
scaler = StandardScaler()

# ã‚¹ãƒ†ãƒƒãƒ—1: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®ã¿çµ±è¨ˆå€¤ã‚’å­¦ç¿’
print("\nã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆå€¤ã‚’å­¦ç¿’")
# fit()ãƒ¡ã‚½ãƒƒãƒ‰ã§è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¹³å‡ãƒ»æ¨™æº–åå·®ã‚’è¨ˆç®—ãƒ»è¨˜éŒ²
scaler.fit(X_train[numeric_features])        # âš ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼

# å­¦ç¿’ã—ãŸçµ±è¨ˆå€¤ã‚’ç¢ºèª
print("å­¦ç¿’ã—ãŸå¹³å‡å€¤:")
for i, col in enumerate(numeric_features):
    print(f"  {col}: {scaler.mean_[i]:.2f}")  # å„ç‰¹å¾´é‡ã®å¹³å‡å€¤

print("å­¦ç¿’ã—ãŸæ¨™æº–åå·®:")
for i, col in enumerate(numeric_features):
    print(f"  {col}: {scaler.scale_[i]:.2f}") # å„ç‰¹å¾´é‡ã®æ¨™æº–åå·®

# ã‚¹ãƒ†ãƒƒãƒ—2: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
print("\nã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›")
# transform()ãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚¹ãƒ†ãƒƒãƒ—1ã§å­¦ç¿’ã—ãŸçµ±è¨ˆå€¤ã‚’ä½¿ç”¨ã—ã¦å¤‰æ›
X_train_scaled = X_train.copy()              # å…ƒãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ä½œæˆ
X_train_scaled[numeric_features] = scaler.transform(X_train[numeric_features])

# å¤‰æ›å¾Œã®çµ±è¨ˆå€¤ç¢ºèªï¼ˆç†è«–çš„ã«ã¯å¹³å‡â‰ˆ0, æ¨™æº–åå·®â‰ˆ1ï¼‰
print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–å¾Œã®çµ±è¨ˆå€¤:")
for col in numeric_features:
    mean_val = X_train_scaled[col].mean()     # æ­£è¦åŒ–å¾Œã®å¹³å‡
    std_val = X_train_scaled[col].std()       # æ­£è¦åŒ–å¾Œã®æ¨™æº–åå·®
    print(f"  {col}: å¹³å‡={mean_val:.4f}, æ¨™æº–åå·®={std_val:.4f}")

# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åŒã˜çµ±è¨ˆå€¤ã§å¤‰æ›
print("\nã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åŒã˜çµ±è¨ˆå€¤ã§å¤‰æ›")
# âš ï¸ é‡è¦ï¼šåŒã˜scalerã‚’ä½¿ç”¨ï¼ˆæ–°ãŸã«fit()ã—ãªã„ï¼‰
X_test_scaled = X_test.copy()                # å…ƒãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ä½œæˆ
X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–å¾Œçµ±è¨ˆå€¤ï¼ˆè¨“ç·´ã¨ç•°ãªã‚‹å€¤ã«ãªã‚‹å ´åˆãŒã‚ã‚‹ï¼‰
print("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–å¾Œã®çµ±è¨ˆå€¤:")
for col in numeric_features:
    mean_val = X_test_scaled[col].mean()      # æ­£è¦åŒ–å¾Œã®å¹³å‡
    std_val = X_test_scaled[col].std()        # æ­£è¦åŒ–å¾Œã®æ¨™æº–åå·®
    print(f"  {col}: å¹³å‡={mean_val:.4f}, æ¨™æº–åå·®={std_val:.4f}")

print("\nâœ… æ­£è¦åŒ–å®Œäº†ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚å¤‰æ›æ¸ˆã¿")
```

---

## ğŸš« ã‚ˆãã‚ã‚‹é–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³

### âŒ é–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³1: å…¨ãƒ‡ãƒ¼ã‚¿ã§æ­£è¦åŒ–

```python
print("\n=== âŒ é–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³1: å…¨ãƒ‡ãƒ¼ã‚¿ã§æ­£è¦åŒ– ===")

# ã“ã‚Œã¯çµ¶å¯¾ã«ã‚„ã£ã¦ã¯ã„ã‘ãªã„ï¼
wrong_scaler = StandardScaler()

# ğŸš¨ å±é™ºï¼šå…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆXï¼‰ã§çµ±è¨ˆå€¤ã‚’å­¦ç¿’
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ãŒè¨“ç·´ã«æ¼ã‚Œã‚‹
print("å±é™ºãªæ“ä½œï¼šå…¨ãƒ‡ãƒ¼ã‚¿ã§æ­£è¦åŒ–...")
X_all_wrong = pd.concat([X_train, X_test])   # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
wrong_scaler.fit(X_all_wrong[numeric_features])  # å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’

# ã“ã®å¾Œã§åˆ†å‰²ã—ã¦ã‚‚æ‰‹é…ã‚Œ
X_train_wrong = wrong_scaler.transform(X_train[numeric_features])
X_test_wrong = wrong_scaler.transform(X_test[numeric_features])

print("ğŸš¨ å•é¡Œç‚¹ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ãŒè¨“ç·´ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹")
print("æœ¬ç•ªç’°å¢ƒã§ã¯æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã¯å­˜åœ¨ã—ãªã„ãŸã‚ã€ç¾å®Ÿçš„ã§ãªã„è©•ä¾¡ã¨ãªã‚‹")
```

### âŒ é–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’

```python
print("\n=== âŒ é–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’ ===")

# ã“ã‚Œã‚‚é–“é•ã„ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§åˆ¥ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½œã‚‹
test_scaler = StandardScaler()               # æ–°ã—ã„ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä½œæˆ

# ğŸš¨ å±é™ºï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æ–°ãŸã«çµ±è¨ˆå€¤ã‚’å­¦ç¿’
test_scaler.fit(X_test[numeric_features])    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
X_test_wrong2 = test_scaler.transform(X_test[numeric_features])

print("ğŸš¨ å•é¡Œç‚¹ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã®çµ±è¨ˆå€¤ã§å¤‰æ›ã—ã¦ã„ã‚‹")
print("è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã§ç•°ãªã‚‹åŸºæº–ã‚’ä½¿ã†ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãå‹•ä½œã—ãªã„")
```

### âœ… æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šçµ±ä¸€ã•ã‚ŒãŸå¤‰æ›

```python
print("\n=== âœ… æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šçµ±ä¸€ã•ã‚ŒãŸå¤‰æ› ===")

# æ­£ã—ã„æ–¹æ³•ã®å†ç¢ºèª
correct_scaler = StandardScaler()

# 1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’
correct_scaler.fit(X_train[numeric_features])

# 2. åŒã˜çµ±è¨ˆå€¤ã§ä¸¡æ–¹ã‚’å¤‰æ›
X_train_correct = correct_scaler.transform(X_train[numeric_features])
X_test_correct = correct_scaler.transform(X_test[numeric_features])

print("âœ… æ­£è§£ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ã§ä¸¡æ–¹ã‚’çµ±ä¸€å¤‰æ›")
print("æœ¬ç•ªç’°å¢ƒã§ã‚‚åŒã˜çµ±è¨ˆå€¤ã‚’ä½¿ç”¨ã§ãã‚‹ãŸã‚ã€ç¾å®Ÿçš„ãªè©•ä¾¡")
```

---

## ğŸ¯ å®Ÿè·µä¾‹ï¼šå®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡

```python
print("\n=== ãƒ•ã‚§ãƒ¼ã‚º4: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ===")

# Random Forestãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
model = RandomForestRegressor(
    n_estimators=100,                        # æ±ºå®šæœ¨ã®æ•°
    random_state=42,                         # å†ç¾æ€§ç¢ºä¿
    n_jobs=-1                               # å…¨CPUã‚³ã‚¢ã‚’ä½¿ç”¨
)

# æ­£è¦åŒ–æ¸ˆã¿è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
print("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’é–‹å§‹...")
model.fit(X_train_scaled[numeric_features], y_train)
print("âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")

# ç‰¹å¾´é‡é‡è¦åº¦ã®ç¢ºèª
feature_importance = pd.DataFrame({
    'feature': numeric_features,             # ç‰¹å¾´é‡å
    'importance': model.feature_importances_ # é‡è¦åº¦ã‚¹ã‚³ã‚¢
}).sort_values('importance', ascending=False)  # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ

print("\nç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½5ä½ï¼‰:")
print(feature_importance.head())

print("\n=== ãƒ•ã‚§ãƒ¼ã‚º5: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ ===")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ï¼ˆéå­¦ç¿’ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
y_train_pred = model.predict(X_train_scaled[numeric_features])
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ï¼ˆçœŸã®æ€§èƒ½è©•ä¾¡ï¼‰
y_test_pred = model.predict(X_test_scaled[numeric_features])
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®RMSE: {train_rmse:.2f}")
print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®RMSE: {test_rmse:.2f}")

# éå­¦ç¿’ã®åˆ¤å®š
overfitting_ratio = test_rmse / train_rmse
if overfitting_ratio > 1.2:
    print(f"âš ï¸  éå­¦ç¿’ã®å¯èƒ½æ€§ã‚ã‚Šï¼ˆæ¯”ç‡: {overfitting_ratio:.2f}ï¼‰")
elif overfitting_ratio < 0.8:
    print(f"âš ï¸  ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®å¯èƒ½æ€§ï¼ˆæ¯”ç‡: {overfitting_ratio:.2f}ï¼‰")
else:
    print(f"âœ… é©åˆ‡ãªå­¦ç¿’çŠ¶æ…‹ï¼ˆæ¯”ç‡: {overfitting_ratio:.2f}ï¼‰")
```

### äºˆæ¸¬çµæœã®å¯è¦–åŒ–

```python
print("\n=== äºˆæ¸¬çµæœã®å¯è¦–åŒ– ===")

# æ•£å¸ƒå›³ã§å®Ÿéš›å€¤vsäºˆæ¸¬å€¤ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(12, 5))                 # å›³ã®ã‚µã‚¤ã‚ºè¨­å®š

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµæœ
plt.subplot(1, 2, 1)                        # 1è¡Œ2åˆ—ã®1ç•ªç›®
plt.scatter(y_train, y_train_pred, alpha=0.6, color='blue')
plt.plot([y_train.min(), y_train.max()],    # å¯¾è§’ç·šï¼ˆå®Œå…¨äºˆæ¸¬ãƒ©ã‚¤ãƒ³ï¼‰
         [y_train.min(), y_train.max()], 'r--', lw=2)
plt.xlabel('å®Ÿéš›ã®å¹´å')
plt.ylabel('äºˆæ¸¬å¹´å')
plt.title(f'è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆRMSE: {train_rmse:.2f}ï¼‰')
plt.grid(True, alpha=0.3)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµæœ
plt.subplot(1, 2, 2)                        # 1è¡Œ2åˆ—ã®2ç•ªç›®
plt.scatter(y_test, y_test_pred, alpha=0.6, color='green')
plt.plot([y_test.min(), y_test.max()],      # å¯¾è§’ç·šï¼ˆå®Œå…¨äºˆæ¸¬ãƒ©ã‚¤ãƒ³ï¼‰
         [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('å®Ÿéš›ã®å¹´å')
plt.ylabel('äºˆæ¸¬å¹´å')
plt.title(f'ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆRMSE: {test_rmse:.2f}ï¼‰')
plt.grid(True, alpha=0.3)

plt.tight_layout()                          # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
plt.show()

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆäºˆæ¸¬èª¤å·®ã®åˆ†æï¼‰
plt.figure(figsize=(10, 4))

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ®‹å·®
plt.subplot(1, 2, 1)
train_residuals = y_train - y_train_pred    # æ®‹å·®è¨ˆç®—
plt.scatter(y_train_pred, train_residuals, alpha=0.6, color='blue')
plt.axhline(y=0, color='r', linestyle='--')  # ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³
plt.xlabel('äºˆæ¸¬å¹´å')
plt.ylabel('æ®‹å·®ï¼ˆå®Ÿéš›å€¤ - äºˆæ¸¬å€¤ï¼‰')
plt.title('è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
plt.grid(True, alpha=0.3)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ®‹å·®
plt.subplot(1, 2, 2)
test_residuals = y_test - y_test_pred       # æ®‹å·®è¨ˆç®—
plt.scatter(y_test_pred, test_residuals, alpha=0.6, color='green')
plt.axhline(y=0, color='r', linestyle='--')  # ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³
plt.xlabel('äºˆæ¸¬å¹´å')
plt.ylabel('æ®‹å·®ï¼ˆå®Ÿéš›å€¤ - äºˆæ¸¬å€¤ï¼‰')
plt.title('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## ğŸ”§ å®Ÿç”¨çš„ãªTips

### 1. sklearn Pipelineã®æ´»ç”¨

```python
print("\n=== Pipelineæ´»ç”¨ã«ã‚ˆã‚‹è‡ªå‹•åŒ– ===")

from sklearn.pipeline import Pipeline

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å‰å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆ
pipeline = Pipeline([
    ('scaler', StandardScaler()),           # ã‚¹ãƒ†ãƒƒãƒ—1: æ­£è¦åŒ–
    ('model', RandomForestRegressor(random_state=42))  # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«
])

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
print("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å­¦ç¿’é–‹å§‹...")
pipeline.fit(X_train[numeric_features], y_train)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆæ­£è¦åŒ–ã‚‚è‡ªå‹•å®Ÿè¡Œï¼‰
y_pred_pipeline = pipeline.predict(X_test[numeric_features])
pipeline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_pipeline))

print(f"Pipelineã‚’ä½¿ã£ãŸå ´åˆã®RMSE: {pipeline_rmse:.2f}")
print("âœ… Pipelineä½¿ç”¨ã«ã‚ˆã‚Šã€æ­£è¦åŒ–å¿˜ã‚Œã®ãƒŸã‚¹ã‚’é˜²æ­¢")
```

### 2. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®åˆ†å‰²

```python
print("\n=== æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®æ³¨æ„ç‚¹ ===")

# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ åˆ†å‰²ã§ã¯ãªãæ™‚é–“é †åˆ†å‰²
def time_series_split(df, test_ratio=0.2):
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚é–“é †ã«åˆ†å‰²ã™ã‚‹é–¢æ•°
    """
    n_total = len(df)                       # å…¨ãƒ‡ãƒ¼ã‚¿æ•°
    n_train = int(n_total * (1 - test_ratio))  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°è¨ˆç®—
    
    # æ™‚é–“é †ã«åˆ†å‰²ï¼ˆå¤ã„ãƒ‡ãƒ¼ã‚¿â†’è¨“ç·´ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿â†’ãƒ†ã‚¹ãƒˆï¼‰
    train_data = df.iloc[:n_train]          # å‰åŠã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    test_data = df.iloc[n_train:]           # å¾ŒåŠã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    
    return train_data, test_data

# ä½¿ç”¨ä¾‹ï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
if 'date' in df.columns:                   # æ—¥ä»˜åˆ—ãŒã‚ã‚‹å ´åˆ
    df_sorted = df.sort_values('date')      # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
    train_ts, test_ts = time_series_split(df_sorted)
    print(f"æ™‚ç³»åˆ—åˆ†å‰²ï¼šè¨“ç·´{len(train_ts)}è¡Œ, ãƒ†ã‚¹ãƒˆ{len(test_ts)}è¡Œ")
```

### 3. æ­£è¦åŒ–ã®é€†å¤‰æ›

```python
print("\n=== æ­£è¦åŒ–ã®é€†å¤‰æ›ï¼ˆå®Ÿç”¨ä¾‹ï¼‰===")

# äºˆæ¸¬çµæœã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã—ãŸã„å ´åˆï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®æ­£è¦åŒ–ï¼‰
y_scaler = StandardScaler()

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚‚æ­£è¦åŒ–ã™ã‚‹å ´åˆ
y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®æ­£è¦åŒ–çµ±è¨ˆå€¤:")
print(f"å¹³å‡: {y_scaler.mean_[0]:.2f}")
print(f"æ¨™æº–åå·®: {y_scaler.scale_[0]:.2f}")

# æ­£è¦åŒ–æ¸ˆã¿ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
model_scaled_target = RandomForestRegressor(random_state=42)
model_scaled_target.fit(X_train_scaled[numeric_features], y_train_scaled)

# äºˆæ¸¬ã¨é€†å¤‰æ›
y_pred_scaled = model_scaled_target.predict(X_test_scaled[numeric_features])
# inverse_transform()ã§å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
y_pred_original = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()

print(f"\né€†å¤‰æ›å¾Œã®äºˆæ¸¬ä¾‹ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
for i in range(5):
    print(f"å®Ÿéš›å€¤: {y_test.iloc[i]:.0f}, äºˆæ¸¬å€¤: {y_pred_original[i]:.0f}")
```

---

## ğŸ“‹ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å®Ÿè£…å‰ã®ç¢ºèª

```python
def pre_implementation_check(X_train, X_test, y_train, y_test):
    """
    å®Ÿè£…å‰ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ…‹ç¢ºèªé–¢æ•°
    """
    print("=== å®Ÿè£…å‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ ===")
    
    checks = {
        "ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æ¸ˆã¿": X_train is not None and X_test is not None,
        "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°åˆ†å‰²æ¸ˆã¿": y_train is not None and y_test is not None,
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºé©åˆ‡": len(X_train) > len(X_test),
        "æ¬ æå€¤ãªã—ï¼ˆè¨“ç·´ï¼‰": not X_train.isnull().any().any(),
        "æ¬ æå€¤ãªã—ï¼ˆãƒ†ã‚¹ãƒˆï¼‰": not X_test.isnull().any().any(),
    }
    
    for check_name, result in checks.items():
        status = "âœ…" if result else "âŒ"
        print(f"{status} {check_name}")
    
    return all(checks.values())

# ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
all_checks_passed = pre_implementation_check(X_train, X_test, y_train, y_test)
if all_checks_passed:
    print("\nğŸ‰ ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹ï¼æ­£è¦åŒ–ã‚’é–‹å§‹ã§ãã¾ã™")
else:
    print("\nâš ï¸  ã„ãã¤ã‹ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä¿®æ­£ã—ã¦ã‹ã‚‰æ­£è¦åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
```

### å®Ÿè£…å¾Œã®ç¢ºèª

```python
def post_implementation_check(scaler, X_train_scaled, X_test_scaled):
    """
    æ­£è¦åŒ–å®Ÿè£…å¾Œã®ç¢ºèªé–¢æ•°
    """
    print("\n=== å®Ÿè£…å¾Œãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ ===")
    
    # æ•°å€¤ç‰¹å¾´é‡ã®ã¿æŠ½å‡º
    numeric_cols = X_train_scaled.select_dtypes(include=[np.number]).columns
    
    checks = {
        "ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãŒå­¦ç¿’æ¸ˆã¿": hasattr(scaler, 'mean_'),
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿å¹³å‡â‰ˆ0": abs(X_train_scaled[numeric_cols].mean().mean()) < 0.1,
        "è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¨™æº–åå·®â‰ˆ1": abs(X_train_scaled[numeric_cols].std().mean() - 1) < 0.1,
        "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ä¸€è‡´": X_train_scaled.shape[1] == X_test_scaled.shape[1],
    }
    
    for check_name, result in checks.items():
        status = "âœ…" if result else "âŒ"
        print(f"{status} {check_name}")
    
    # è©³ç´°çµ±è¨ˆã®è¡¨ç¤º
    print(f"\nğŸ“Š æ­£è¦åŒ–å¾Œçµ±è¨ˆå€¤:")
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å¹³å‡: {X_train_scaled[numeric_cols].mean().mean():.4f}")
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¨™æº–åå·®: {X_train_scaled[numeric_cols].std().mean():.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å¹³å‡: {X_test_scaled[numeric_cols].mean().mean():.4f}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¨™æº–åå·®: {X_test_scaled[numeric_cols].std().mean():.4f}")
    
    return all(checks.values())

# ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
post_checks_passed = post_implementation_check(scaler, X_train_scaled, X_test_scaled)
if post_checks_passed:
    print("\nğŸ‰ æ­£è¦åŒ–ãŒæ­£ã—ãå®Ÿè¡Œã•ã‚Œã¾ã—ãŸï¼")
else:
    print("\nâš ï¸  æ­£è¦åŒ–ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å†ç¢ºèªã—ã¦ãã ã•ã„")
```

---

## ğŸš€ æœ¬ç•ªç’°å¢ƒã§ã®é‹ç”¨

### æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬

```python
print("\n=== æœ¬ç•ªç’°å¢ƒã§ã®æ–°ãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ ===")

def predict_new_data(model, scaler, new_data, feature_columns):
    """
    æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬é–¢æ•°ï¼ˆæœ¬ç•ªç’°å¢ƒç”¨ï¼‰
    """
    print("æ–°ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬å‡¦ç†é–‹å§‹...")
    
    # 1. åŒã˜å‰å‡¦ç†ã‚’é©ç”¨
    new_data_scaled = new_data.copy()
    new_data_scaled[feature_columns] = scaler.transform(new_data[feature_columns])
    
    # 2. äºˆæ¸¬å®Ÿè¡Œ
    predictions = model.predict(new_data_scaled[feature_columns])
    
    # 3. çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¿”ã™
    result_df = new_data.copy()
    result_df['predicted_salary'] = predictions
    
    print(f"âœ… {len(new_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’äºˆæ¸¬å®Œäº†")
    return result_df

# æ–°ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ï¼ˆå®Ÿéš›ã«ã¯APIã‚„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ï¼‰
new_employee_data = pd.DataFrame({
    'age': [30, 35, 28],
    'experience': [5, 8, 3],
    'education_score': [85, 92, 78]
})

print("æ–°å…¥ç¤¾å“¡ãƒ‡ãƒ¼ã‚¿:")
print(new_employee_data)

# äºˆæ¸¬å®Ÿè¡Œ
if 'model' in locals() and 'scaler' in locals():
    prediction_result = predict_new_data(
        model, scaler, new_employee_data, numeric_features
    )
    print("\näºˆæ¸¬çµæœ:")
    print(prediction_result[['age', 'experience', 'predicted_salary']])
```

---

## ğŸ“š ã¾ã¨ã‚

### ğŸ¯ é‡è¦ãƒã‚¤ãƒ³ãƒˆã®å†ç¢ºèª

1. **ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãŒå…ˆã€æ­£è¦åŒ–ãŒå¾Œ**: ã“ã®é †åºã¯çµ¶å¯¾ã«å®ˆã‚‹
2. **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§çµ±è¨ˆå€¤å­¦ç¿’**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã¯ä½¿ã‚ãªã„
3. **åŒã˜ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã§ä¸¡æ–¹å¤‰æ›**: çµ±ä¸€ã•ã‚ŒãŸåŸºæº–ã§æ­£è¦åŒ–
4. **Pipelineã®æ´»ç”¨**: æ‰‹é †ã®è‡ªå‹•åŒ–ã§ãƒŸã‚¹é˜²æ­¢
5. **ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®æ´»ç”¨**: ç¢ºå®Ÿãªå®Ÿè£…ã®ãŸã‚ã®æ¤œè¨¼

### ğŸ”„ å…¸å‹çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```python
# æ¨å¥¨ã•ã‚Œã‚‹å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
def complete_ml_pipeline(df, target_column, test_size=0.2):
    """
    å®Œå…¨ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """
    # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X = df.drop(target_column, axis=1)
    y = df[target_column]
    
    # 2. åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    # 3. æ­£è¦åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', RandomForestRegressor(random_state=42))
    ])
    
    # 4. å­¦ç¿’
    pipeline.fit(X_train, y_train)
    
    # 5. è©•ä¾¡
    y_pred = pipeline.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    return pipeline, rmse

print("ğŸ‰ ã“ã‚Œã§å®Œç’§ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒã‚¹ã‚¿ãƒ¼ã—ã¾ã—ãŸï¼")
```

**æ–°äººã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®çš†ã•ã‚“ã€ã“ã®é †åºã‚’å®ˆã£ã¦å®Ÿè£…ã™ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ãªã„ä¿¡é ¼æ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§æ´»ç”¨ã—ã¦ãã ã•ã„ï¼**
