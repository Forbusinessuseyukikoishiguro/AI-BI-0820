# TableauとPythonで学ぶデータ正規化チートシート
## 新人エンジニア向け完全ガイド

---

## 🔄 機械学習パイプラインでの正規化

### 📋 正しい処理フロー図

```
【データ準備フェーズ】
┌─────────────────┐
│   生データ       │
│ (CSV, DB, etc.) │
└─────────┬───────┘
          │
          ▼
┌─────────────────┐
│ データクリーニング │
│ (欠損値処理等)    │
└─────────┬───────┘
          │
          ▼
┌─────────────────┐
│ 訓練/テスト分割  │
│ train_test_split │
└─────────┬───────┘
          │
          ▼
【正規化フェーズ】⚠️ 重要：この順序を守る！
┌─────────────────┐
│ ① 訓練データで   │
│   統計値を学習   │
│   scaler.fit()  │
└─────────┬───────┘
          │
          ▼
┌─────────────────┐
│ ② 訓練データを   │
│   変換           │
│ scaler.transform │
└─────────┬───────┘
          │
          ▼
┌─────────────────┐
│ ③ テストデータを │
│   同じ統計値で   │
│   変換           │
│ scaler.transform │
└─────────┬───────┘
          │
          ▼
【モデル学習・評価】
┌─────────────────┐
│ モデル訓練       │
└─────────┬───────┘
          │
          ▼
┌─────────────────┐
│ モデル評価       │
└─────────────────┘
```

### ⚠️ データリーク防止の重要ポイント

```
❌ 間違った順序 (データリークあり)
├── 全データで正規化
├── 訓練/テスト分割
└── モデル学習
    └── 結果: テストデータの情報が訓練に漏れる

✅ 正しい順序 (データリークなし)  
├── 訓練/テスト分割
├── 訓練データのみで正規化パラメータ学習
├── 学習したパラメータでテストデータ変換
└── モデル学習
    └── 結果: 真の汎化性能を測定可能
```

## 📚 データ正規化とは？

データ正規化とは、異なるスケールや単位のデータを統一された範囲に変換する処理です。機械学習やデータ分析において、データの特徴量間の影響を均等にするために重要な前処理手法です。

### 📊 正規化の概念図

```
【正規化前】スケールがバラバラ
年齢: 25, 35, 28, 42, 31        （20-50の範囲）
年収: 350万, 620万, 410万, 780万, 530万  （300-800万の範囲）
経験: 2, 12, 5, 18, 8年         （0-20の範囲）

            ↓ 正規化処理 ↓

【正規化後】統一されたスケール
Min-Max: すべて 0.0 〜 1.0 の範囲
Z-score: 平均0、標準偏差1の分布
Robust: 中央値0を中心とした分布
```

### なぜ正規化が必要？

```
🎯 機械学習での影響例

【正規化前】
年収(万円): [350, 620, 410, 780, 530]  ← 大きな数値が支配的
年齢(歳):   [25,  35,  28,  42,  31]   ← 小さな数値は無視される

【結果】距離計算で年収の影響が過大になる！

【正規化後】  
年収: [0.0, 0.63, 0.14, 1.0, 0.42]  ← 同じスケール
年齢: [0.0, 0.59, 0.18, 1.0, 0.35]  ← 同じスケール

【結果】すべての特徴量が平等に評価される✅
```

- **スケールの違い**: 年収（数百万円）と年齢（数十歳）など、単位が異なるデータを比較可能にする
- **アルゴリズムの精度向上**: 距離ベースのアルゴリズム（KNN、SVM等）では必須
- **学習の安定化**: 勾配降下法での収束を早める

---

## 🎯 主要な正規化手法

### 📈 視覚的比較図

```
元データ: [10, 20, 30, 40, 50]

┌─────────────────────────────────────────┐
│ Min-Max正規化 (0-1スケーリング)           │
│ 公式: (x - min) / (max - min)           │
│ 結果: [0.0, 0.25, 0.5, 0.75, 1.0]      │
│ 特徴: 必ず0〜1の範囲に収まる             │
└─────────────────────────────────────────┘

┌─────────────────────────────────────────┐
│ Z-score正規化 (標準化)                  │
│ 公式: (x - 平均) / 標準偏差             │
│ 結果: [-1.26, -0.63, 0, 0.63, 1.26]    │
│ 特徴: 平均0、標準偏差1の正規分布         │
└─────────────────────────────────────────┘

┌─────────────────────────────────────────┐
│ Robust正規化                           │
│ 公式: (x - 中央値) / (Q3 - Q1)         │
│ 結果: [-1.0, -0.5, 0, 0.5, 1.0]       │
│ 特徴: 外れ値に影響されにくい             │
└─────────────────────────────────────────┘
```

### 1. Min-Max正規化（0-1スケーリング）
```
正規化後 = (元の値 - 最小値) / (最大値 - 最小値)
```
**特徴**: 0から1の範囲に変換

### 2. Z-score正規化（標準化）
```
正規化後 = (元の値 - 平均値) / 標準偏差
```
**特徴**: 平均0、標準偏差1の正規分布に変換

### 3. Robust正規化
```
正規化後 = (元の値 - 中央値) / (75%点 - 25%点)
```
**特徴**: 外れ値に対して頑健

---

## 🐍 Python（pandas）での実装

### 環境準備

```python
# pandasライブラリをインポート（Excel風のデータ操作ができる）
import pandas as pd
# numpyライブラリをインポート（数値計算・配列操作用）
import numpy as np
# sklearnから正規化用のクラスをインポート
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
# matplotlibをインポート（グラフ作成用）
import matplotlib.pyplot as plt
# seabornをインポート（統計的グラフの美しい可視化用）
import seaborn as sns

# matplotlib の日本語フォント設定（文字化け防止）
plt.rcParams['font.family'] = 'DejaVu Sans'
```

### サンプルデータの作成

```python
# numpyの乱数生成器のシードを固定（毎回同じ結果を得るため）
np.random.seed(42)

# Pythonの辞書型でサンプルデータを作成
data = {
    # 'name'キーに文字列のリストを格納（正規化対象外）
    'name': ['田中', '佐藤', '山田', '鈴木', '高橋'],
    # 'age'キーに年齢データを格納（20-50の小さい数値範囲）
    'age': [25, 35, 28, 42, 31],
    # 'salary'キーに年収データを格納（数百万円の大きい数値範囲）
    'salary': [3500000, 6200000, 4100000, 7800000, 5300000],
    # 'experience'キーに経験年数を格納（0-20の中程度数値範囲）
    'experience': [2, 12, 5, 18, 8]
}

# pandasのDataFrameクラスのコンストラクタに辞書を渡してデータフレーム作成
df = pd.DataFrame(data)

# print関数で文字列を出力（コンソールに表示される）
print("元データ:")
# データフレーム全体をprint関数で出力（表形式で表示される）
print(df)
```

### 1. Min-Max正規化の実装

```python
# print関数で区切り線を表示（視覚的にセクションを分ける）
print("\n=== Min-Max正規化 ===")

# リスト型で正規化対象の列名を指定（文字列の列は除外）
numeric_columns = ['age', 'salary', 'experience']

# MinMaxScalerクラスのインスタンス（オブジェクト）を作成
scaler_minmax = MinMaxScaler()

# copy()メソッドでデータフレームの完全なコピーを作成（元データ保護）
df_minmax = df.copy()
# fit_transform()メソッドで学習と変換を同時実行
# 学習：各列の最小値・最大値を記録
# 変換：(値-最小値)/(最大値-最小値)の計算を実行
# []演算子で指定した列のみを抽出・変換し、同じ列に代入
df_minmax[numeric_columns] = scaler_minmax.fit_transform(df[numeric_columns])

# print関数で結果表示用の文字列を出力
print("Min-Max正規化後:")
# 正規化後のデータフレーム全体をコンソールに表示
print(df_minmax)

# print関数で統計情報の見出しを表示
print("\n正規化後の統計情報:")
# describe()メソッドで基本統計量（平均、標準偏差、最小値、最大値等）を計算・表示
print(df_minmax[numeric_columns].describe())
```

### 2. Z-score正規化（標準化）の実装

```python
# print関数でZ-score正規化セクションの開始を表示
print("\n=== Z-score正規化 ===")

# StandardScalerクラスのインスタンスを作成（平均0、標準偏差1に変換）
scaler_standard = StandardScaler()

# copy()メソッドで元データの完全コピーを作成
df_standard = df.copy()
# fit_transform()メソッドで学習と変換を同時実行
# 学習：各列の平均値・標準偏差を計算して記録
# 変換：(値-平均値)/標準偏差 の計算を全データに適用
# []演算子で数値列のみを指定し、変換結果を同じ列に代入
df_standard[numeric_columns] = scaler_standard.fit_transform(df[numeric_columns])

# print関数で標準化結果の見出しを表示
print("Z-score正規化後:")
# 標準化後のデータフレーム全体をコンソールに出力
print(df_standard)

# print関数で平均値確認の見出しを表示
print("\n正規化後の平均値:")
# mean()メソッドで各列の平均値を計算（理論的には0に近い値になる）
print(df_standard[numeric_columns].mean())

# print関数で標準偏差確認の見出しを表示
print("\n正規化後の標準偏差:")
# std()メソッドで各列の標準偏差を計算（理論的には1に近い値になる）
print(df_standard[numeric_columns].std())
```

### 3. Robust正規化の実装

```python
# print関数でRobust正規化セクションの開始を表示
print("\n=== Robust正規化 ===")

# RobustScalerクラスのインスタンスを作成（外れ値に頑健な正規化）
scaler_robust = RobustScaler()

# copy()メソッドで元データの完全コピーを作成（元データ保護）
df_robust = df.copy()
# fit_transform()メソッドで学習と変換を同時実行
# 学習：各列の中央値（Q2）と四分位範囲（Q3-Q1）を計算・記録
# 変換：(値-中央値)/(Q3-Q1) の計算を全データに適用
# []演算子で数値列のみを指定し、変換結果を同じ列に代入
df_robust[numeric_columns] = scaler_robust.fit_transform(df[numeric_columns])

# print関数でロバスト正規化結果の見出しを表示
print("Robust正規化後:")
# ロバスト正規化後のデータフレーム全体をコンソールに出力
print(df_robust)

# print関数で中央値確認の見出しを表示
print("\n正規化後の中央値:")
# median()メソッドで各列の中央値を計算（理論的には0に近い値になる）
print(df_robust[numeric_columns].median())
```

### 4. 手動実装（理解を深める）

```python
# print関数で手動実装セクションの開始を表示
print("\n=== 手動Min-Max正規化 ===")

# def文で関数を定義（manual_minmax_normalize という名前の関数）
def manual_minmax_normalize(series):
    """
    Min-Max正規化を手動で実装する関数
    引数：series（pandas Series型のデータ）
    戻り値：正規化済みのpandas Series
    """
    # min()メソッドで系列の最小値を取得・変数に保存
    min_val = series.min()
    # max()メソッドで系列の最大値を取得・変数に保存
    max_val = series.max()
    # Min-Max正規化の数式を適用：(各値 - 最小値) / (最大値 - 最小値)
    # pandas Seriesの演算は各要素に自動的に適用される（ベクトル化演算）
    normalized = (series - min_val) / (max_val - min_val)
    # return文で正規化後のSeriesを関数の戻り値として返す
    return normalized

# copy()メソッドで元データの完全コピーを作成
df_manual = df.copy()

# for文で数値列のリストをループ処理（各列名が変数colに格納される）
for col in numeric_columns:
    # f文字列（f"..."）で変数colの値を文字列に埋め込んで表示
    print(f"\n{col}列の正規化:")
    # []演算子で指定列のデータを取得・変数に保存
    original_values = df[col]
    # 定義した関数を呼び出してMin-Max正規化を実行
    normalized_values = manual_minmax_normalize(original_values)
    # 正規化後の値を手動データフレームの該当列に代入
    df_manual[col] = normalized_values
    
    # print関数で最小値を表示（min()メソッドの結果）
    print(f"最小値: {original_values.min()}")
    # print関数で最大値を表示（max()メソッドの結果）
    print(f"最大値: {original_values.max()}")
    # tolist()メソッドでpandas SeriesをPythonリストに変換して表示
    print(f"正規化前: {original_values.tolist()}")
    # round(3)で小数点3桁に丸めてからtolist()でリスト変換・表示
    print(f"正規化後: {normalized_values.round(3).tolist()}")

# print関数で最終結果の見出しを表示
print("\n手動正規化後の全データ:")
# 手動で正規化したデータフレーム全体をコンソールに出力
print(df_manual)
```

### 5. 可視化で理解を深める

```python
# print関数で可視化セクションの開始を表示
print("\n=== 可視化による比較 ===")

# figure()関数で新しい図（Figure）を作成・サイズ指定（幅15インチ×高さ10インチ）
plt.figure(figsize=(15, 10))

# subplot()関数で2行2列のサブプロット配置の1番目を作成・選択
plt.subplot(2, 2, 1)
# for文で数値列をループ処理（各列を個別に描画）
for col in numeric_columns:
    # plot()関数で折れ線グラフを描画
    # df.index：行番号（0,1,2,3,4）をX軸に使用
    # df[col]：指定列の値をY軸に使用
    # marker='o'：各点を円形マーカーで表示
    # label=col：凡例用のラベル名を指定
    plt.plot(df.index, df[col], marker='o', label=col)
# title()関数でサブプロットのタイトルを設定
plt.title('正規化前のデータ')
# legend()関数で凡例（各線の説明）を表示
plt.legend()
# grid(True)関数でグリッド線を表示（グラフの読み取りやすさ向上）
plt.grid(True)

# subplot()関数で2行2列のサブプロット配置の2番目を作成・選択
plt.subplot(2, 2, 2)
# for文で数値列をループ処理
for col in numeric_columns:
    # plot()関数でMin-Max正規化後のデータを描画
    # df_minmax.index：正規化後データの行番号をX軸
    # df_minmax[col]：正規化後の指定列をY軸
    # marker='s'：各点を正方形マーカーで表示（元データと区別）
    plt.plot(df_minmax.index, df_minmax[col], marker='s', label=col)
# title()関数でサブプロットのタイトルを設定
plt.title('Min-Max正規化後')
# legend()関数で凡例を表示
plt.legend()
# grid(True)関数でグリッド線を表示
plt.grid(True)

# subplot()関数で2行2列のサブプロット配置の3番目を作成・選択
plt.subplot(2, 2, 3)
# for文で数値列をループ処理
for col in numeric_columns:
    # plot()関数でZ-score正規化後のデータを描画
    # marker='^'：各点を三角形マーカーで表示（他の手法と区別）
    plt.plot(df_standard.index, df_standard[col], marker='^', label=col)
# title()関数でサブプロットのタイトルを設定
plt.title('Z-score正規化後')
# legend()関数で凡例を表示
plt.legend()
# grid(True)関数でグリッド線を表示
plt.grid(True)

# subplot()関数で2行2列のサブプロット配置の4番目を作成・選択
plt.subplot(2, 2, 4)
# for文で数値列をループ処理
for col in numeric_columns:
    # plot()関数でRobust正規化後のデータを描画
    # marker='d'：各点をダイヤモンド形マーカーで表示
    plt.plot(df_robust.index, df_robust[col], marker='d', label=col)
# title()関数でサブプロットのタイトルを設定
plt.title('Robust正規化後')
# legend()関数で凡例を表示
plt.legend()
# grid(True)関数でグリッド線を表示
plt.grid(True)

# tight_layout()関数で各サブプロット間の間隔を自動調整
plt.tight_layout()
# show()関数で作成したグラフを画面に表示
plt.show()

# figure()関数で新しい図を作成・サイズ指定（幅12インチ×高さ8インチ）
plt.figure(figsize=(12, 8))

# enumerate()関数でリストの要素とインデックスを同時取得
for i, col in enumerate(numeric_columns):
    # subplot()関数で2行3列配置のサブプロット作成
    # i+1：インデックスは0から始まるが、サブプロット番号は1から始まるため
    plt.subplot(2, 3, i+1)
    # hist()関数でヒストグラム（度数分布）を描画
    # df[col]：元データの指定列
    # alpha=0.5：透明度50%（重なりを見やすくする）
    # label='元データ'：凡例用ラベル
    # bins=10：データを10個の区間に分割
    plt.hist(df[col], alpha=0.5, label='元データ', bins=10)
    # hist()関数で正規化後データのヒストグラムを重ね描き
    plt.hist(df_minmax[col], alpha=0.5, label='Min-Max', bins=10)
    # title()関数で各列の分布比較タイトルを設定
    plt.title(f'{col}の分布比較')
    # legend()関数で凡例を表示
    plt.legend()
    # grid()関数で薄いグリッド線を表示（alpha=0.3で透明度70%）
    plt.grid(True, alpha=0.3)

# tight_layout()関数でレイアウトを自動調整
plt.tight_layout()
# show()関数でヒストグラム群を画面に表示
plt.show()
```

### 6. 実際のデータ分析での活用例

```python
# print関数で相関分析セクションの開始を表示
print("\n=== 相関分析での正規化効果 ===")

# print関数で正規化前の相関係数見出しを表示
print("正規化前の相関係数:")
# corr()メソッドで相関係数行列を計算（-1から1の値、線形関係の強さを表す）
# []演算子で数値列のみを抽出してから相関係数計算
corr_before = df[numeric_columns].corr()
# round(3)メソッドで小数点3桁に丸めて表示
print(corr_before.round(3))

# print関数で正規化後の相関係数見出しを表示
print("\nMin-Max正規化後の相関係数:")
# 正規化後データで相関係数行列を計算
corr_after = df_minmax[numeric_columns].corr()
# round(3)メソッドで小数点3桁に丸めて表示
print(corr_after.round(3))

# print関数で相関係数の差確認見出しを表示
print("\n相関係数の差（理論的には0に近い値）:")
# 正規化前後の相関係数行列の差を計算（正規化は相関係数を変えない）
# round(6)メソッドで小数点6桁まで表示（微小な誤差確認）
print((corr_after - corr_before).round(6))

# figure()関数で新しい図を作成・サイズ指定（幅12インチ×高さ5インチ）
plt.figure(figsize=(12, 5))

# subplot()関数で1行2列配置の1番目サブプロットを作成・選択
plt.subplot(1, 2, 1)
# heatmap()関数で相関係数行列をヒートマップ（色付き行列）として描画
# annot=True：各セルに数値を表示
# cmap='coolwarm'：青（負の相関）から赤（正の相関）のカラーマップ
# center=0：カラーマップの中心を0に設定
sns.heatmap(corr_before, annot=True, cmap='coolwarm', center=0)
# title()関数でサブプロットのタイトルを設定
plt.title('正規化前の相関係数')

# subplot()関数で1行2列配置の2番目サブプロットを作成・選択
plt.subplot(1, 2, 2)
# 正規化後の相関係数行列をヒートマップで描画
sns.heatmap(corr_after, annot=True, cmap='coolwarm', center=0)
# title()関数でサブプロットのタイトルを設定
plt.title('正規化後の相関係数')

# tight_layout()関数でサブプロット間の間隔を自動調整
plt.tight_layout()
# show()関数でヒートマップを画面に表示
plt.show()
```

---

## 📊 Tableauでのデータ正規化

### 🖥️ 計算フィールド作成手順図

```
Tableau画面での操作手順:

┌─────────────────────────────────────────┐
│ ① メニューバー                           │
│ [分析] → [計算フィールドの作成]           │
└─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────┐
│ ② 計算フィールドエディタが開く            │
│ ┌─────────────────────────────────────┐ │
│ │ 名前: [売上_正規化]                  │ │
│ ├─────────────────────────────────────┤ │
│ │ 計算式入力エリア:                   │ │
│ │                                   │ │
│ │ ([売上] - {FIXED : MIN([売上])}) / │ │
│ │ ({FIXED : MAX([売上])} -           │ │
│ │  {FIXED : MIN([売上])})            │ │
│ │                                   │ │
│ └─────────────────────────────────────┘ │
│ [OK] [キャンセル]                       │
└─────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────┐
│ ③ フィールドが作成され、                  │
│   左パネルのメジャーに追加される          │
│                                       │
│ メジャー                               │
│ ├── 売上                              │
│ ├── 売上_正規化 ← 新しく作成           │
│ └── その他のメジャー                    │
└─────────────────────────────────────────┘
```

### 📐 LOD (Level of Detail) 計算の解説図

```
{FIXED : MIN([売上])} の動作イメージ:

通常のMIN([売上])の場合:
┌──────────────────┐  ┌──────────────────┐
│ 地域A            │  │ 地域B            │
│ ├── 店舗1: 100万 │  │ ├── 店舗3: 80万  │
│ └── 店舗2: 150万 │  │ └── 店舗4: 120万 │
│                  │  │                  │
│ MIN = 100万      │  │ MIN = 80万       │
└──────────────────┘  └──────────────────┘
     ↑グループごとに別々の最小値

{FIXED : MIN([売上])}の場合:
┌────────────────────────────────────────┐
│           全データ統一                  │
│ 店舗1: 100万, 店舗2: 150万              │
│ 店舗3: 80万,  店舗4: 120万              │
│                                      │
│ FIXED MIN = 80万 (全体で一つの値)      │
└────────────────────────────────────────┘
     ↑全体で単一の最小値を使用
```

### 1. 計算フィールドによるMin-Max正規化

```sql
// Min-Max正規化の計算フィールド
// フィールド名: [売上_正規化]

([売上] - {FIXED : MIN([売上])}) / 
({FIXED : MAX([売上])} - {FIXED : MIN([売上])})

// 解説:
// - [売上]: 正規化したい元のフィールド
// - {FIXED : MIN([売上])}: データ全体の最小値
// - {FIXED : MAX([売上])}: データ全体の最大値
// - 結果は0から1の範囲になる
```

### 2. Z-score正規化（標準化）

```sql
// Z-score正規化の計算フィールド
// フィールド名: [売上_標準化]

([売上] - {FIXED : AVG([売上])}) / 
{FIXED : STDEV([売上])}

// 解説:
// - {FIXED : AVG([売上])}: データ全体の平均値
// - {FIXED : STDEV([売上])}: データ全体の標準偏差
// - 結果は平均0、標準偏差1の分布になる
```

### 3. パーセンタイル正規化

```sql
// パーセンタイル正規化の計算フィールド
// フィールド名: [売上_パーセンタイル]

RANK_PERCENTILE([売上])

// 解説:
// - 各値のパーセンタイル順位を計算
// - 0から1の範囲で表現
// - 外れ値に対して頑健
```

### 4. 実践的なTableauワークフロー

#### Step 1: データソースの接続
```
1. Tableauを起動
2. データソース（Excel, CSV, データベース等）に接続
3. データプレビューで数値フィールドを確認
```

#### Step 2: 計算フィールドの作成
```
1. [分析] → [計算フィールドの作成]
2. 上記の正規化式を入力
3. フィールド名を分かりやすく設定
```

#### Step 3: 可視化での活用
```
1. 正規化前後のデータを散布図で比較
2. ダッシュボードに正規化効果を表示
3. フィルターで正規化方法を切り替え可能にする
```

---

## 🎯 実践での使い分けガイド

### 📊 正規化手法選択フローチャート

```
データの特性を確認
        │
        ▼
┌─────────────────┐
│ 外れ値はある？   │
└─────┬───────────┘
      │
  ┌───▼────┐    ┌──────▼──────┐
  │ 多い   │    │ 少ない/なし  │
  └───┬────┘    └──────┬──────┘
      │                │
      ▼                ▼
┌─────────────────┐ ┌─────────────────┐
│ Robust正規化    │ │ 分布の形は？     │
│ を推奨          │ └─────┬───────────┘
└─────────────────┘       │
                    ┌─────▼──────┐
                    │ 正規分布に   │
                    │ 近い？       │
                    └─────┬──────┘
                          │
                   ┌──────▼───────┐
                   │ Yes    │ No   │
                   └──────┬─┬─────┘
                          │ │
                          ▼ ▼
              ┌─────────────┐ ┌─────────────┐
              │ Z-score正規化│ │ Min-Max正規化│
              │ (標準化)     │ │ (0-1スケール)│
              └─────────────┘ └─────────────┘
```

### 📈 各手法の適用例

```
【Min-Max正規化】を使う場面
┌────────────────────────────────────────┐
│ ✅ 適用例                              │
│ • テストスコア (0-100点)               │
│ • アンケート評価 (1-5段階)             │
│ • 画像のピクセル値 (0-255)             │
│ • 確率や割合のデータ                   │
│                                      │
│ 📊 特徴                               │
│ • 必ず0-1の範囲に収まる                │
│ • 直感的で解釈しやすい                  │
│ • 外れ値の影響を受けやすい               │
└────────────────────────────────────────┘

【Z-score正規化】を使う場面  
┌────────────────────────────────────────┐
│ ✅ 適用例                              │
│ • 身長・体重などの生体データ             │
│ • 金融データ (株価、為替等)             │
│ • センサーデータ                       │
│ • ニューラルネットワークの入力           │
│                                      │
│ 📊 特徴                               │
│ • 平均0、標準偏差1の標準正規分布         │
│ • 統計的手法との相性が良い               │
│ • 正規分布データで最も効果的             │
└────────────────────────────────────────┘

【Robust正規化】を使う場面
┌────────────────────────────────────────┐
│ ✅ 適用例                              │
│ • 年収データ (高額所得者が少数存在)      │
│ • Webアクセス数 (バイラル的な急増あり)   │
│ • 売上データ (季節要因で大きく変動)      │
│ • ソーシャルメディアの反応数             │
│                                      │
│ 📊 特徴                               │
│ • 外れ値に影響されにくい                │
│ • 中央値と四分位範囲を使用               │
│ • 歪んだ分布でも安定した結果             │
└────────────────────────────────────────┘
```

---

## ⚠️ 注意点とベストプラクティス

### 🚫 よくある間違いと正しい対処法

```
【間違い①】全データで正規化してから分割
┌─────────────────────────────────────────┐
│ ❌ 危険なパターン                        │
│                                       │
│ 全データ(1000件)                       │
│      │                               │
│      ▼                               │
│ ┌─────────────┐                       │
│ │ 正規化実行   │ ← 全データの統計値使用  │
│ │ fit_transform│                     │
│ └─────────────┘                       │
│      │                               │
│      ▼                               │
│ 訓練(800件) | テスト(200件)            │
│                                       │
│ 🔥 問題: テストデータの情報が            │
│          訓練に漏れている！              │
└─────────────────────────────────────────┘

【正解】分割してから正規化
┌─────────────────────────────────────────┐
│ ✅ 正しいパターン                        │
│                                       │
│ 全データ(1000件)                       │
│      │                               │
│      ▼                               │
│ 訓練(800件) | テスト(200件)            │
│      │           │                   │
│      ▼           │                   │
│ ┌─────────────┐  │                   │
│ │ fit()で学習  │  │                   │
│ │ 統計値記録   │  │                   │
│ └─────────────┘  │                   │
│      │           │                   │
│      ▼           ▼                   │
│ ┌─────────────┐ ┌─────────────┐      │
│ │transform()で│ │transform()で│      │
│ │訓練データ変換│ │テストデータ │      │
│ │             │ │変換          │      │
│ └─────────────┘ └─────────────┘      │
│                                       │
│ ✅ 解決: 訓練データの統計値のみ使用      │
└─────────────────────────────────────────┘
```

### 🔄 逆変換の重要性

```
【モデル予測結果の解釈】

正規化済みモデルの予測値: 0.75
        │
        ▼
┌─────────────────┐
│ inverse_transform│ ← 逆変換で元のスケールに戻す
│ 0.75 → 625万円   │
└─────────────────┘
        │
        ▼
業務担当者に報告: "予測年収は625万円です"

📝 ポイント:
• 正規化された値(0.75)では解釈困難
• 元のスケール(625万円)なら業務で理解可能
• inverse_transform()メソッドで簡単に変換
```

### ⚡ パフォーマンス最適化

```
【大量データ処理のアプローチ】

従来の方法 (メモリ不足のリスク):
┌────────────────────────────────────┐
│ 大量データ(10GB)                   │
│       │                          │
│       ▼                          │
│ ┌──────────────┐                 │
│ │ 全データを   │ ← メモリ不足！   │
│ │ メモリに読込 │                 │
│ └──────────────┘                 │
└────────────────────────────────────┘

改善された方法 (チャンク処理):
┌────────────────────────────────────┐
│ 大量データ(10GB)                   │
│       │                          │
│       ▼                          │
│ ┌──────────────┐ ┌──────────────┐ │
│ │ チャンク1    │ │ チャンク2    │ │
│ │ (100MB)     │ │ (100MB)     │ │
│ └──────────────┘ └──────────────┘ │
│       │               │          │
│       ▼               ▼          │
│ ┌──────────────┐ ┌──────────────┐ │
│ │ 並列処理     │ │ 並列処理     │ │
│ │ 正規化実行   │ │ 正規化実行   │ │
│ └──────────────┘ └──────────────┘ │
│                                  │
│ ✅ メモリ効率的 + 高速処理        │
└────────────────────────────────────┘
```

### よくある間違い

```python
# ❌ 間違った例：テストデータで正規化パラメータを学習してしまう
# StandardScalerクラスのインスタンスを作成（テスト用）
test_scaler = StandardScaler()
# fit_transform()でテストデータから平均・標準偏差を学習して変換
# これはデータリーク！テストデータの情報が訓練に漏れてしまう
test_normalized = test_scaler.fit_transform(X_test)

# ✅ 正しい例：訓練データでのみパラメータを学習し、テストデータに適用
# StandardScalerクラスのインスタンスを作成（訓練用）
train_scaler = StandardScaler()
# fit_transform()で訓練データから平均・標準偏差を学習し、同時に変換
train_normalized = train_scaler.fit_transform(X_train)
# transform()で学習済みパラメータ（訓練データの平均・標準偏差）をテストデータに適用
# テストデータからは新たに統計値を学習しない
test_normalized = train_scaler.transform(X_test)
```

### データリークの防止

```python
# sklearn.model_selectionモジュールからTimeSeriesSplitクラスをインポート
from sklearn.model_selection import TimeSeriesSplit

# def文で時系列データ用の正規化関数を定義
def time_series_normalize(data, train_size):
    """
    時系列データの正規化処理（データリーク防止版）
    引数：
      data: 正規化対象のデータフレームまたは配列
      train_size: 訓練データのサイズ（行数）
    戻り値：
      train_normalized: 正規化済み訓練データ
      test_normalized: 正規化済みテストデータ
      scaler: 学習済みスケーラーオブジェクト
    """
    # スライス記法で時系列データを時間順に分割
    # [:train_size]：開始から train_size 行目まで（訓練データ）
    train_data = data[:train_size]
    # [train_size:]：train_size 行目以降すべて（テストデータ）
    test_data = data[train_size:]
    
    # StandardScalerクラスのインスタンスを作成
    scaler = StandardScaler()
    # fit()メソッドで訓練データのみから平均値・標準偏差を学習
    # テストデータの情報は一切使用しない（データリーク防止）
    scaler.fit(train_data)
    
    # transform()メソッドで学習済みパラメータを訓練データに適用
    train_normalized = scaler.transform(train_data)
    # transform()メソッドで同じパラメータをテストデータに適用
    # テストデータから新たに統計値を計算しない
    test_normalized = scaler.transform(test_data)
    
    # return文で3つの値をタプルとして返す
    return train_normalized, test_normalized, scaler

# 関数を呼び出して時系列正規化を実行
# 引数：数値列のみのデータフレーム、訓練データサイズ3行
train_norm, test_norm, fitted_scaler = time_series_normalize(df[numeric_columns], 3)
# print関数で処理完了メッセージを表示
print("時系列正規化完了")
```

### 正規化後の逆変換

```python
# print関数で逆変換セクションの開始を表示
print("\n=== 正規化の逆変換 ===")

# []演算子で給与列のみを抽出、values属性でnumpy配列に変換
original_data = df[['salary']].values
# MinMaxScalerクラスのインスタンスを作成
scaler = MinMaxScaler()
# fit_transform()メソッドで学習と正規化を同時実行
# 元データの最小値・最大値を学習し、0-1範囲に変換
normalized_data = scaler.fit_transform(original_data)

# print関数で元データ表示の見出しを出力
print("元データ:")
# flatten()メソッドで2次元配列を1次元配列に変換して表示
print(original_data.flatten())

# print関数で正規化後データ表示の見出しを出力
print("正規化後:")
# flatten()で1次元化、round(3)で小数点3桁に丸めて表示
print(normalized_data.flatten().round(3))

# inverse_transform()メソッドで正規化を逆変換（元のスケールに戻す）
# 学習時に記録した最小値・最大値を使用して復元
restored_data = scaler.inverse_transform(normalized_data)

# print関数で逆変換後データ表示の見出しを出力
print("逆変換後:")
# flatten()メソッドで1次元化して復元されたデータを表示
print(restored_data.flatten())

# 元データと逆変換後データの差を計算（数値誤差の確認）
difference = original_data - restored_data
# print関数で差の確認見出しを表示
print("差（理論的には0）:")
# flatten()で1次元化、round(10)で小数点10桁まで表示（極小誤差確認）
print(difference.flatten().round(10))
```

---

## 📈 パフォーマンス最適化

### 大量データでの効率的な正規化

```python
# daskライブラリのdataframeモジュールをddという別名でインポート
import dask.dataframe as dd

# def文で大量データ用の効率的正規化関数を定義
def efficient_normalize_large_data(file_path):
    """
    大量データの効率的な正規化処理関数
    メモリ使用量を抑えながら並列処理で高速化
    
    引数：
      file_path: 処理対象CSVファイルのパス（文字列）
    戻り値：
      normalized_df: 正規化済みDaskデータフレーム
    """
    # read_csv()関数でCSVファイルを並列読み込み（メモリ効率的）
    # Daskは大量データを複数のパーティションに分割して処理
    df_large = dd.read_csv(file_path)
    
    # 空の辞書を作成（各列の統計値を格納する）
    stats = {}
    # for文で数値列をループ処理
    for col in numeric_columns:
        # 辞書に各列の統計値を格納（ネストした辞書構造）
        stats[col] = {
            # min()で最小値計算、compute()で実際に計算実行
            'min': df_large[col].min().compute(),
            # max()で最大値計算、compute()で実際に計算実行
            'max': df_large[col].max().compute(),
            # mean()で平均値計算、compute()で実際に計算実行
            'mean': df_large[col].mean().compute(),
            # std()で標準偏差計算、compute()で実際に計算実行
            'std': df_large[col].std().compute()
        }
    
    # def文でチャンク（データの一部分）処理用の内部関数を定義
    def normalize_chunk(chunk):
        """
        データの一部分（チャンク）を正規化する関数
        引数：chunk（pandasデータフレームの一部）
        戻り値：正規化済みチャンク
        """
        # copy()メソッドでチャンクをコピー（元データ保護）
        result = chunk.copy()
        # for文で数値列をループ処理
        for col in numeric_columns:
            # 事前計算した統計値辞書から最小値を取得
            min_val = stats[col]['min']
            # 事前計算した統計値辞書から最大値を取得
            max_val = stats[col]['max']
            # Min-Max正規化の数式を適用
            # (各値 - 最小値) / (最大値 - 最小値)
            result[col] = (chunk[col] - min_val) / (max_val - min_val)
        # return文で正規化済みチャンクを返す
        return result
    
    # map_partitions()メソッドで各パーティションに正規化関数を並列適用
    # Daskが自動的に複数CPUコアで並列処理を実行
    normalized_df = df_large.map_partitions(normalize_chunk)
    
    # return文で正規化済みDaskデータフレームを返す
    return normalized_df

# print関数で関数定義完了メッセージを表示
print("大量データ処理関数を定義しました")
```

---

## 🔍 まとめ

### 🎓 新人エンジニア学習ロードマップ

```
【学習ステップ】

レベル1: 基礎理解 📚
┌─────────────────────────────────────────┐
│ □ データ正規化の必要性を理解             │
│ □ 3つの主要手法の違いを把握              │
│ □ 簡単なサンプルデータで実践             │
│ □ Pythonの基本的な実装をマスター          │
└─────────────────────────────────────────┘
          │
          ▼
レベル2: 実践応用 🛠️
┌─────────────────────────────────────────┐
│ □ データリーク防止の重要性を理解         │
│ □ train/test分割後の正規化実装           │
│ □ Tableauでの計算フィールド作成          │
│ □ 可視化による効果確認                   │
└─────────────────────────────────────────┘
          │
          ▼
レベル3: 高度な技術 🚀
┌─────────────────────────────────────────┐
│ □ 大量データでの効率的処理               │
│ □ パイプライン化（sklearn Pipeline）     │
│ □ カスタム正規化手法の実装               │
│ □ 業務データでの実践プロジェクト         │
└─────────────────────────────────────────┘
          │
          ▼
レベル4: エキスパート 🎯
┌─────────────────────────────────────────┐
│ □ 特徴量エンジニアリング全般             │
│ □ AutoML での自動正規化                  │
│ □ 深層学習での Batch Normalization      │
│ □ チーム教育・ベストプラクティス普及     │
└─────────────────────────────────────────┘
```

### 📋 チェックリスト

```
実装前の確認事項:
┌─────────────────────────────────────────┐
│ □ データの分布を確認した                 │
│ □ 外れ値の有無をチェックした             │
│ □ 適切な正規化手法を選択した             │
│ □ train/test分割順序を確認した           │
│ □ 逆変換の必要性を検討した               │
└─────────────────────────────────────────┘

実装後の確認事項:
┌─────────────────────────────────────────┐
│ □ 正規化後の統計値を確認した             │
│ □ 分布の変化を可視化した                 │
│ □ 相関係数が保持されていることを確認     │
│ □ パフォーマンスへの影響を測定した       │
│ □ 業務担当者向けの解釈を準備した         │
└─────────────────────────────────────────┘
```

### 新人エンジニアが覚えるべきポイント

1. **目的に応じた手法選択**: データの特性を理解して適切な正規化を選ぶ
2. **データリーク防止**: 訓練データでのみパラメータを学習し、テストデータに適用
3. **可視化での確認**: 正規化前後の分布を必ず確認する
4. **逆変換の保持**: モデル結果を元のスケールで解釈するため

### 次のステップ
- 特徴量エンジニアリングの学習
- より高度な前処理手法（PCA、特徴選択等）
- 実際のプロジェクトでの実践適用

このチートシートを参考に、データ正規化をマスターして分析精度を向上させましょう！
