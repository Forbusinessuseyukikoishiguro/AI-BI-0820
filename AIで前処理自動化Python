"""
Tableau Excel ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ„ãƒ¼ãƒ«
Author: Claude
Description: Excelãƒ‡ãƒ¼ã‚¿ã‚’Tableauåˆ†æç”¨ã«åŠ¹ç‡çš„ã«å‰å‡¦ç†ã™ã‚‹ãŸã‚ã®Pythonãƒ„ãƒ¼ãƒ«
"""

import pandas as pd
import numpy as np
import os
import re
from datetime import datetime
from typing import List, Dict, Optional, Union, Any
import warnings
warnings.filterwarnings('ignore')


class TableauDataPreprocessor:
    """
    Tableauåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹
    
    Excel/CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¤‰æ›ã‚’è¡Œã†
    """
    
    def __init__(self, file_path: str = None):
        """
        åˆæœŸåŒ–
        
        Args:
            file_path (str): å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.file_path = file_path
        self.data = None
        self.original_data = None
        self.data_info = {}
        self.processing_log = []
        
        if file_path:
            self.load_data()
    
    def load_data(self, file_path: str = None) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            file_path (str): ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯åˆæœŸåŒ–æ™‚ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼‰
            
        Returns:
            pd.DataFrame: èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        if file_path:
            self.file_path = file_path
            
        if not self.file_path:
            raise ValueError("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.file_path}")
        
        file_extension = os.path.splitext(self.file_path)[1].lower()
        
        try:
            if file_extension in ['.xlsx', '.xls']:
                self.data = pd.read_excel(self.file_path)
            elif file_extension == '.csv':
                # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•åˆ¤å®š
                encodings = ['utf-8', 'shift_jis', 'cp932', 'iso-2022-jp']
                for encoding in encodings:
                    try:
                        self.data = pd.read_csv(self.file_path, encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    self.data = pd.read_csv(self.file_path, encoding='utf-8', errors='ignore')
            else:
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_extension}")
            
            # å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
            self.original_data = self.data.copy()
            self._update_data_info()
            self._log_action(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {os.path.basename(self.file_path)}")
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.data.shape[0]}è¡Œ Ã— {self.data.shape[1]}åˆ—")
            return self.data
            
        except Exception as e:
            raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _update_data_info(self):
        """ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’æ›´æ–°"""
        if self.data is not None:
            self.data_info = {
                'rows': len(self.data),
                'columns': len(self.data.columns),
                'empty_cells': self.data.isnull().sum().sum(),
                'duplicates': self.data.duplicated().sum(),
                'memory_usage': self.data.memory_usage(deep=True).sum(),
                'dtypes': self.data.dtypes.to_dict()
            }
    
    def _log_action(self, action: str):
        """å‡¦ç†ãƒ­ã‚°ã‚’è¨˜éŒ²"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {action}"
        self.processing_log.append(log_entry)
        print(f"ğŸ“ {action}")
    
    def show_data_info(self) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
        
        Returns:
            dict: ãƒ‡ãƒ¼ã‚¿æƒ…å ±
        """
        if self.data is None:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return {}
        
        print("\n" + "="*50)
        print("ğŸ“Š DATA SUMMARY")
        print("="*50)
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(self.file_path) if self.file_path else 'Unknown'}")
        print(f"ğŸ“ ã‚µã‚¤ã‚º: {self.data_info['rows']:,}è¡Œ Ã— {self.data_info['columns']}åˆ—")
        print(f"ğŸ•³ï¸  ç©ºã‚»ãƒ«: {self.data_info['empty_cells']:,}å€‹")
        print(f"ğŸ”„ é‡è¤‡è¡Œ: {self.data_info['duplicates']:,}è¡Œ")
        print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {self.data_info['memory_usage'] / 1024 / 1024:.2f} MB")
        
        print("\nğŸ“‹ åˆ—æƒ…å ±:")
        for i, (col, dtype) in enumerate(self.data_info['dtypes'].items()):
            null_count = self.data[col].isnull().sum()
            null_rate = (null_count / len(self.data)) * 100
            print(f"  {i+1:2d}. {col:<30} | {str(dtype):<10} | æ¬ æ: {null_count:,}å€‹ ({null_rate:.1f}%)")
        
        return self.data_info
    
    def preview_data(self, n_rows: int = 10) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
        
        Args:
            n_rows (int): è¡¨ç¤ºã™ã‚‹è¡Œæ•°
            
        Returns:
            pd.DataFrame: ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return pd.DataFrame()
        
        print(f"\nğŸ‘€ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ (æœ€åˆã®{n_rows}è¡Œ):")
        print("-" * 80)
        preview = self.data.head(n_rows)
        print(preview.to_string(max_cols=None, max_colwidth=20))
        return preview
    
    def remove_empty_rows(self, threshold: float = 0.5) -> pd.DataFrame:
        """
        ç©ºè¡Œã‚’å‰Šé™¤
        
        Args:
            threshold (float): å‰Šé™¤ã™ã‚‹é–¾å€¤ï¼ˆ0.5 = åŠåˆ†ä»¥ä¸ŠãŒç©ºã®è¡Œã‚’å‰Šé™¤ï¼‰
            
        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        initial_rows = len(self.data)
        
        # å„è¡Œã®énullå€¤ã®å‰²åˆã‚’è¨ˆç®—
        non_null_ratio = self.data.notna().sum(axis=1) / len(self.data.columns)
        self.data = self.data[non_null_ratio >= threshold]
        
        removed_rows = initial_rows - len(self.data)
        self._update_data_info()
        self._log_action(f"ç©ºè¡Œå‰Šé™¤: {removed_rows:,}è¡Œå‰Šé™¤ (é–¾å€¤: {threshold})")
        
        return self.data
    
    def remove_empty_columns(self, threshold: float = 0.5) -> pd.DataFrame:
        """
        ç©ºåˆ—ã‚’å‰Šé™¤
        
        Args:
            threshold (float): å‰Šé™¤ã™ã‚‹é–¾å€¤ï¼ˆ0.5 = åŠåˆ†ä»¥ä¸ŠãŒç©ºã®åˆ—ã‚’å‰Šé™¤ï¼‰
            
        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        initial_cols = len(self.data.columns)
        
        # å„åˆ—ã®énullå€¤ã®å‰²åˆã‚’è¨ˆç®—
        non_null_ratio = self.data.notna().sum() / len(self.data)
        cols_to_keep = non_null_ratio[non_null_ratio >= threshold].index
        self.data = self.data[cols_to_keep]
        
        removed_cols = initial_cols - len(self.data.columns)
        self._update_data_info()
        self._log_action(f"ç©ºåˆ—å‰Šé™¤: {removed_cols}åˆ—å‰Šé™¤ (é–¾å€¤: {threshold})")
        
        return self.data
    
    def remove_duplicates(self, subset: List[str] = None, keep: str = 'first') -> pd.DataFrame:
        """
        é‡è¤‡è¡Œã‚’å‰Šé™¤
        
        Args:
            subset (List[str]): é‡è¤‡ãƒã‚§ãƒƒã‚¯ã™ã‚‹åˆ—åã®ãƒªã‚¹ãƒˆï¼ˆNone=å…¨åˆ—ï¼‰
            keep (str): ä¿æŒã™ã‚‹é‡è¤‡è¡Œ ('first', 'last', False)
            
        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        initial_rows = len(self.data)
        self.data = self.data.drop_duplicates(subset=subset, keep=keep)
        
        removed_rows = initial_rows - len(self.data)
        self._update_data_info()
        self._log_action(f"é‡è¤‡è¡Œå‰Šé™¤: {removed_rows:,}è¡Œå‰Šé™¤")
        
        return self.data
    
    def fill_missing_values(self, strategy: str = 'forward', custom_value: Any = None, columns: List[str] = None) -> pd.DataFrame:
        """
        æ¬ æå€¤ã‚’åŸ‹ã‚ã‚‹
        
        Args:
            strategy (str): åŸ‹ã‚ã‚‹æ–¹æ³• ('forward', 'backward', 'mean', 'median', 'mode', 'zero', 'custom')
            custom_value (Any): ã‚«ã‚¹ã‚¿ãƒ å€¤ï¼ˆstrategy='custom'ã®å ´åˆï¼‰
            columns (List[str]): å‡¦ç†ã™ã‚‹åˆ—åã®ãƒªã‚¹ãƒˆï¼ˆNone=å…¨åˆ—ï¼‰
            
        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        initial_nulls = self.data.isnull().sum().sum()
        target_columns = columns if columns else self.data.columns
        
        for col in target_columns:
            if col in self.data.columns:
                if strategy == 'forward':
                    self.data[col] = self.data[col].fillna(method='ffill')
                elif strategy == 'backward':
                    self.data[col] = self.data[col].fillna(method='bfill')
                elif strategy == 'mean' and pd.api.types.is_numeric_dtype(self.data[col]):
                    self.data[col] = self.data[col].fillna(self.data[col].mean())
                elif strategy == 'median' and pd.api.types.is_numeric_dtype(self.data[col]):
                    self.data[col] = self.data[col].fillna(self.data[col].median())
                elif strategy == 'mode':
                    mode_value = self.data[col].mode()
                    if not mode_value.empty:
                        self.data[col] = self.data[col].fillna(mode_value[0])
                elif strategy == 'zero':
                    self.data[col] = self.data[col].fillna(0)
                elif strategy == 'custom':
                    self.data[col] = self.data[col].fillna(custom_value)
        
        final_nulls = self.data.isnull().sum().sum()
        filled_count = initial_nulls - final_nulls
        self._update_data_info()
        self._log_action(f"æ¬ æå€¤å‡¦ç†: {filled_count:,}å€‹ã‚’åŸ‹ã‚ãŸ (æ–¹æ³•: {strategy})")
        
        return self.data
    
    def clean_text_data(self, columns: List[str] = None, operations: List[str] = None) -> pd.DataFrame:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        
        Args:
            columns (List[str]): å‡¦ç†ã™ã‚‹åˆ—åã®ãƒªã‚¹ãƒˆï¼ˆNone=æ–‡å­—åˆ—åˆ—ã™ã¹ã¦ï¼‰
            operations (List[str]): å®Ÿè¡Œã™ã‚‹æ“ä½œã®ãƒªã‚¹ãƒˆ
                ['trim', 'lower', 'upper', 'remove_special', 'normalize_space']
                
        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if operations is None:
            operations = ['trim', 'normalize_space']
        
        # æ–‡å­—åˆ—åˆ—ã‚’è‡ªå‹•é¸æŠ
        if columns is None:
            columns = self.data.select_dtypes(include=['object']).columns.tolist()
        
        processed_columns = []
        
        for col in columns:
            if col in self.data.columns:
                original_data = self.data[col].copy()
                
                for operation in operations:
                    if operation == 'trim':
                        self.data[col] = self.data[col].astype(str).str.strip()
                    elif operation == 'lower':
                        self.data[col] = self.data[col].astype(str).str.lower()
                    elif operation == 'upper':
                        self.data[col] = self.data[col].astype(str).str.upper()
                    elif operation == 'remove_special':
                        self.data[col] = self.data[col].astype(str).str.replace(r'[^\w\s]', '', regex=True)
                    elif operation == 'normalize_space':
                        self.data[col] = self.data[col].astype(str).str.replace(r'\s+', ' ', regex=True)
                
                processed_columns.append(col)
        
        self._update_data_info()
        self._log_action(f"ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°: {len(processed_columns)}åˆ—å‡¦ç† ({', '.join(operations)})")
        
        return self.data
    
    def convert_data_types(self, auto_convert: bool = True, type_mapping: Dict[str, str] = None) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
        
        Args:
            auto_convert (bool): è‡ªå‹•å¤‰æ›ã‚’è¡Œã†ã‹
            type_mapping (dict): æ‰‹å‹•ã§ã®å‹æŒ‡å®š {'column_name': 'new_type'}
            
        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        converted_columns = []
        
        # æ‰‹å‹•å‹æŒ‡å®š
        if type_mapping:
            for col, new_type in type_mapping.items():
                if col in self.data.columns:
                    try:
                        if new_type == 'datetime':
                            self.data[col] = pd.to_datetime(self.data[col], errors='coerce')
                        elif new_type == 'category':
                            self.data[col] = self.data[col].astype('category')
                        else:
                            self.data[col] = self.data[col].astype(new_type)
                        converted_columns.append(f"{col} -> {new_type}")
                    except Exception as e:
                        print(f"âš ï¸ {col}ã®å‹å¤‰æ›ã«å¤±æ•—: {e}")
        
        # è‡ªå‹•å¤‰æ›
        if auto_convert:
            for col in self.data.columns:
                try:
                    # æ•°å€¤å¤‰æ›ã‚’è©¦è¡Œ
                    if self.data[col].dtype == 'object':
                        # æ•°å€¤ã‹ãƒã‚§ãƒƒã‚¯
                        numeric_series = pd.to_numeric(self.data[col], errors='coerce')
                        if numeric_series.notna().sum() / len(self.data[col]) > 0.8:  # 80%ä»¥ä¸ŠãŒæ•°å€¤
                            self.data[col] = numeric_series
                            converted_columns.append(f"{col} -> numeric")
                        
                        # æ—¥ä»˜å¤‰æ›ã‚’è©¦è¡Œ
                        elif not converted_columns or col not in [c.split(' -> ')[0] for c in converted_columns]:
                            try:
                                date_series = pd.to_datetime(self.data[col], errors='coerce')
                                if date_series.notna().sum() / len(self.data[col]) > 0.5:  # 50%ä»¥ä¸ŠãŒæ—¥ä»˜
                                    self.data[col] = date_series
                                    converted_columns.append(f"{col} -> datetime")
                            except:
                                pass
                                
                except Exception:
                    continue
        
        self._update_data_info()
        self._log_action(f"ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›: {len(converted_columns)}åˆ—å¤‰æ›")
        
        return self.data
    
    def filter_data(self, conditions: Dict[str, Any]) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        
        Args:
            conditions (dict): ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶
                ä¾‹: {
                    'column1': {'operator': '>', 'value': 100},
                    'column2': {'operator': 'contains', 'value': 'keyword'},
                    'column3': {'operator': 'in', 'value': ['A', 'B', 'C']}
                }
                
        Returns:
            pd.DataFrame: ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        initial_rows = len(self.data)
        mask = pd.Series([True] * len(self.data))
        
        for column, condition in conditions.items():
            if column not in self.data.columns:
                print(f"âš ï¸ åˆ— '{column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            operator = condition.get('operator', '==')
            value = condition.get('value')
            
            try:
                if operator == '>':
                    mask &= (self.data[column] > value)
                elif operator == '<':
                    mask &= (self.data[column] < value)
                elif operator == '>=':
                    mask &= (self.data[column] >= value)
                elif operator == '<=':
                    mask &= (self.data[column] <= value)
                elif operator == '==':
                    mask &= (self.data[column] == value)
                elif operator == '!=':
                    mask &= (self.data[column] != value)
                elif operator == 'contains':
                    mask &= self.data[column].astype(str).str.contains(str(value), na=False)
                elif operator == 'startswith':
                    mask &= self.data[column].astype(str).str.startswith(str(value), na=False)
                elif operator == 'endswith':
                    mask &= self.data[column].astype(str).str.endswith(str(value), na=False)
                elif operator == 'in':
                    mask &= self.data[column].isin(value if isinstance(value, list) else [value])
                elif operator == 'notin':
                    mask &= ~self.data[column].isin(value if isinstance(value, list) else [value])
                elif operator == 'isnull':
                    mask &= self.data[column].isnull()
                elif operator == 'notnull':
                    mask &= self.data[column].notnull()
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã®é©ç”¨ã«å¤±æ•— ({column}): {e}")
        
        self.data = self.data[mask]
        filtered_rows = initial_rows - len(self.data)
        self._update_data_info()
        self._log_action(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿: {filtered_rows:,}è¡Œé™¤å¤–")
        
        return self.data
    
    def rename_columns(self, column_mapping: Dict[str, str]) -> pd.DataFrame:
        """
        åˆ—åã‚’å¤‰æ›´
        
        Args:
            column_mapping (dict): åˆ—åãƒãƒƒãƒ”ãƒ³ã‚° {'old_name': 'new_name'}
            
        Returns:
            pd.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.data = self.data.rename(columns=column_mapping)
        renamed_count = len([k for k in column_mapping.keys() if k in self.original_data.columns])
        
        self._update_data_info()
        self._log_action(f"åˆ—åå¤‰æ›´: {renamed_count}åˆ—å¤‰æ›´")
        
        return self.data
    
    def create_tableau_extract(self, output_path: str = None, file_format: str = 'excel') -> str:
        """
        Tableauç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        
        Args:
            output_path (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_format (str): ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ ('excel', 'csv')
            
        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if file_format == 'excel':
                output_path = f"tableau_ready_data_{timestamp}.xlsx"
            else:
                output_path = f"tableau_ready_data_{timestamp}.csv"
        
        try:
            if file_format == 'excel':
                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                    # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
                    self.data.to_excel(writer, sheet_name='Data', index=False)
                    
                    # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
                    info_df = pd.DataFrame([
                        ['ç·è¡Œæ•°', self.data_info['rows']],
                        ['ç·åˆ—æ•°', self.data_info['columns']],
                        ['ç©ºã‚»ãƒ«æ•°', self.data_info['empty_cells']],
                        ['é‡è¤‡è¡Œæ•°', self.data_info['duplicates']],
                        ['å‡¦ç†æ—¥æ™‚', datetime.now().strftime("%Y-%m-%d %H:%M:%S")]
                    ], columns=['é …ç›®', 'å€¤'])
                    info_df.to_excel(writer, sheet_name='Summary', index=False)
                    
                    # å‡¦ç†ãƒ­ã‚°
                    if self.processing_log:
                        log_df = pd.DataFrame(self.processing_log, columns=['å‡¦ç†ãƒ­ã‚°'])
                        log_df.to_excel(writer, sheet_name='Processing_Log', index=False)
                        
            elif file_format == 'csv':
                self.data.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            self._log_action(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {output_path}")
            print(f"âœ… Tableauç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
            
            return output_path
            
        except Exception as e:
            raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def reset_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®çŠ¶æ…‹ã«ãƒªã‚»ãƒƒãƒˆ"""
        if self.original_data is not None:
            self.data = self.original_data.copy()
            self._update_data_info()
            self.processing_log = []
            self._log_action("ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆå®Œäº†")
        else:
            print("âŒ å…ƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    def get_processing_log(self) -> List[str]:
        """å‡¦ç†ãƒ­ã‚°ã‚’å–å¾—"""
        return self.processing_log.copy()
    
    def export_processing_summary(self, output_path: str = None) -> str:
        """
        å‡¦ç†ã‚µãƒãƒªãƒ¼ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        Args:
            output_path (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"processing_summary_{timestamp}.txt"
        
        summary = []
        summary.append("=" * 60)
        summary.append("TABLEAU ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚µãƒãƒªãƒ¼")
        summary.append("=" * 60)
        summary.append(f"å‡¦ç†æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        summary.append(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(self.file_path) if self.file_path else 'Unknown'}")
        summary.append("")
        
        summary.append("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
        summary.append(f"  - è¡Œæ•°: {self.data_info['rows']:,}")
        summary.append(f"  - åˆ—æ•°: {self.data_info['columns']}")
        summary.append(f"  - ç©ºã‚»ãƒ«æ•°: {self.data_info['empty_cells']:,}")
        summary.append(f"  - é‡è¤‡è¡Œæ•°: {self.data_info['duplicates']:,}")
        summary.append("")
        
        summary.append("ğŸ”„ å‡¦ç†ãƒ­ã‚°:")
        for log in self.processing_log:
            summary.append(f"  {log}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(summary))
        
        print(f"âœ… å‡¦ç†ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        return output_path


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - ä½¿ç”¨ä¾‹
    """
    print("ğŸš€ Tableau Excelå‰å‡¦ç†ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)
    
    # ä½¿ç”¨ä¾‹
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        # processor = TableauDataPreprocessor("sample_data.xlsx")
        
        # ã¾ãŸã¯ç©ºã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã¦ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        processor = TableauDataPreprocessor()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
        file_path = input("ğŸ“ å‡¦ç†ã™ã‚‹Excel/CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
        if file_path and os.path.exists(file_path):
            processor.load_data(file_path)
            
            # ãƒ‡ãƒ¼ã‚¿æƒ…å ±è¡¨ç¤º
            processor.show_data_info()
            processor.preview_data()
            
            # åŸºæœ¬çš„ãªå‰å‡¦ç†ã‚’å®Ÿè¡Œ
            print("\nğŸ§¹ åŸºæœ¬ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹...")
            processor.remove_empty_rows()
            processor.remove_empty_columns()
            processor.remove_duplicates()
            processor.clean_text_data()
            processor.convert_data_types()
            
            # Tableauç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            output_file = processor.create_tableau_extract()
            
            # å‡¦ç†ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
            processor.export_processing_summary()
            
            print(f"\nâœ… å‰å‡¦ç†å®Œäº†ï¼ {output_file} ã‚’Tableauã§èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
            
        else:
            print("âŒ æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()


# =============================================================================
# è©³ç´°ä½¿ç”¨ä¾‹
# =============================================================================

"""
ä½¿ç”¨ä¾‹1: åŸºæœ¬çš„ãªå‰å‡¦ç†
------------------------
processor = TableauDataPreprocessor("data.xlsx")
processor.show_data_info()
processor.remove_empty_rows()
processor.remove_duplicates()
processor.clean_text_data()
processor.convert_data_types(auto_convert=True)
processor.create_tableau_extract("clean_data.xlsx")


ä½¿ç”¨ä¾‹2: ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†
--------------------
processor = TableauDataPreprocessor("sales_data.csv")

# æ¬ æå€¤ã‚’å¹³å‡å€¤ã§åŸ‹ã‚ã‚‹
processor.fill_missing_values(strategy='mean', columns=['price', 'quantity'])

# ç‰¹å®šã®æ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿
conditions = {
    'sales_amount': {'operator': '>', 'value': 1000},
    'region': {'operator': 'in', 'value': ['Tokyo', 'Osaka']}
}
processor.filter_data(conditions)

# åˆ—åã‚’å¤‰æ›´
column_mapping = {
    'old_column_name': 'new_column_name',
    'price': 'unit_price'
}
processor.rename_columns(column_mapping)

# CSVå½¢å¼ã§ä¿å­˜
processor.create_tableau_extract("filtered_sales.csv", file_format='csv')


ä½¿ç”¨ä¾‹3: å‹å¤‰æ›æŒ‡å®š
------------------
type_mapping = {
    'date_column': 'datetime',
    'category_column': 'category',
    'number_column': 'float64'
}
processor.convert_data_types(auto_convert=False, type_mapping=type_mapping)


ä½¿ç”¨ä¾‹4: ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
---------------------------
operations = ['trim', 'lower', 'remove_special', 'normalize_space']
processor.clean_text_data(columns=['name', 'description'], operations=operations)
"""
